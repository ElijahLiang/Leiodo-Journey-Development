[{"content":"Core Concepts Black Box System Theory is a methodology for understanding complex systems by focusing on inputs and outputs rather than internal mechanisms. It originated in cybernetics and systems theory, with wide applications in science, engineering, philosophy, and cognitive science.\nCore Principle When we cannot (or need not) understand the internal structure of a system, we can still effectively interact with it by observing the relationship between inputs and outputs.\nTheoretical Framework System Structure I n p u t ‚Üí [ B ( l U ‚ñà a n ‚ñà c k ‚ñà k n ‚ñà o ‚ñà B w ‚ñà o n ‚ñà x ‚ñà I n ] t e ‚Üí r n O a u l t p S u t t a t e ) Core Elements Element Description Input Controllable variables entering the system Output Observable results produced by the system Black Box Internal mechanism (may be unknown/irrelevant) Feedback Information used to adjust inputs Dynamic Interaction Loop S u b j ‚Üë ‚Üê e ‚Üê c ‚Üê t ‚Üê ‚Üí S h D a y p n e a s m i S c u b I j n e t c e t r a ‚Üê c ‚Üê t ‚Üê i ‚Üê o ‚Üê n ‚Üê ‚Üê ‚Üí ‚Üê ‚Üê O ‚Üê u ‚Üê t ‚Üê p ‚Üê u ‚Üê t ‚Üê ‚Üê / ‚Üì P a r t o f P r o c e s s Applications 1. Scientific Research Behaviorism: Study psychology by observing stimulus-response without needing to understand brain mechanisms.\nPharmacology: Test drug efficacy by observing effects without fully understanding molecular mechanisms.\n2. Engineering API Design: Users only need to know input/output formats, not implementation details.\nMachine Learning: We can use neural networks effectively without fully understanding why they work.\n3. Philosophy of Mind Consciousness Problem: We can study the relationship between behavior and mental states without solving the hard problem of consciousness.\nTuring Test: Judge intelligence by behavior (output), not internal mechanisms.\n4. Everyday Life Social Interaction: We predict others\u0026rsquo; behavior through observation without fully understanding their thought processes.\nTool Usage: We can use computers effectively without understanding every circuit.\nSymbol Legend Symbol Meaning ‚ñà Black Box ‚Üí Input / Injection Direction ‚Üî Bidirectional Interaction ? Unknown Output / Algorithm ‚ü≥ Dynamic Loop Key Insights Pragmatic Focus: What matters is effectiveness, not complete understanding Complexity Reduction: Simplifies interaction with complex systems Iterative Learning: Improve understanding through feedback loops Boundary Recognition: Know what you can control vs. what you can only observe Further Reading Cybernetics - Norbert Wiener An Introduction to Cybernetics - W. Ross Ashby The Black Box: A Record of the Disaster - Amos Oz Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/knowledge/black-box-system-theory/","summary":"A framework for understanding and interacting with complex systems when internal mechanisms are unknown or irrelevant.","title":"Black Box System Theory"},{"content":"Core Concept Block Universe Theory (also called Eternalism) is a perspective in modern physics about the nature of time. It proposes that past, present, and future all exist simultaneously in a four-dimensional spacetime structure, where time is simply one of the dimensions.\nFour-Dimensional Spacetime Structure F 4 P N u D a o t s w u S t r p e a c e t i E F ‚òÖ S E W m v i u v a e e x W b e i n e h j n t ( t d e e t i B s , r c s n l e t g o t u i t c h n c v h f k a c o e a o t h n t r U a s m n o n c o e a i c g i m x w v c e o e i a e u a u n s r r r b s t t e s r l n n e e e e e ) d s s s s Key Insights All moments exist simultaneously: Like all pages of a book existing at once, all moments of the universe exist simultaneously in the 4D spacetime block.\nConsciousness is a 0-dimensional point: Our consciousness can only experience \u0026ldquo;now\u0026rdquo; - like a projector\u0026rsquo;s beam illuminating only one frame of film.\nTime flow is subjective: What we perceive as \u0026ldquo;time passing\u0026rdquo; is actually consciousness moving through the 4D block, scanning frame by frame.\nDimensional Hierarchy Dimension Type Degrees of Freedom Experience Space 3D (x, y, z) Free movement Can move in any direction Time 1D (t) One-way only Can only move past‚Üífuture Experience 0D (point) None Can only experience \u0026ldquo;now\u0026rdquo; Cognition 4D+ Understanding Can imagine higher dimensions Three Views of Time 1. Presentism View: Only the present is real; the past has vanished, the future doesn\u0026rsquo;t exist yet.\nProblems:\nConflicts with relativity (different reference frames have different \u0026ldquo;nows\u0026rdquo;) Cannot explain the nature of causality 2. Growing Block Universe View: Past and present are real, future doesn\u0026rsquo;t exist yet; universe is constantly \u0026ldquo;growing.\u0026rdquo;\nProblems:\nRequires defining an absolute \u0026ldquo;now\u0026rdquo; Not fully compatible with relativity 3. Block Universe ‚òÖ View: Past, present, and future are equally real, all existing in the 4D spacetime block.\nAdvantages:\nFully compatible with relativity Explains time\u0026rsquo;s symmetry Matches physics\u0026rsquo; mathematical descriptions Practical Implications 1. Rethinking Time Management Traditional: Time is flowing, we must \u0026ldquo;catch\u0026rdquo; it.\nBlock Universe: Time doesn\u0026rsquo;t flow; our consciousness moves along the time axis.\nInsight: Focus on the present‚Äîit\u0026rsquo;s the only moment consciousness can directly experience.\n2. Alleviating Anxiety Traditional anxiety: Worrying about future, regretting past.\nBlock Universe perspective:\nPast is already part of the spacetime block‚Äîcannot change, no need to regret Future in some sense already \u0026ldquo;exists\u0026rdquo;‚Äîwe just haven\u0026rsquo;t experienced it yet Present is the only moment where we can \u0026ldquo;choose\u0026rdquo; 3. Reconsidering Life\u0026rsquo;s Meaning Traditional: Life is brief, will eventually vanish.\nBlock Universe: Your life exists eternally in the 4D spacetime block; every moment has eternal value.\nScientific Basis Einstein\u0026rsquo;s Spacetime View Einstein unified time and space into four-dimensional spacetime in relativity. Time is no longer an independent flow but forms a geometric structure together with space.\nEinstein\u0026rsquo;s quote (in a letter to the family of his friend Besso):\n\u0026ldquo;For us believing physicists, the distinction between past, present and future is only a stubbornly persistent illusion.\u0026rdquo;\nMinkowski Spacetime Mathematician Minkowski expressed relativity as 4D spacetime geometry, where time is the fourth dimension:\nSpacetime Interval:\nŒî s ¬≤ = c ¬≤ Œî t ¬≤ - Œî x ¬≤ - Œî y ¬≤ - Œî z ¬≤ Further Reading The Order of Time - Carlo Rovelli Relativity - Albert Einstein A Brief History of Time - Stephen Hawking Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/knowledge/block-universe-theory/","summary":"Understanding time as a four-dimensional structure where past, present, and future all exist simultaneously.","title":"Block Universe Theory (Eternalism)"},{"content":"Core Philosophy Complex systems become unmanageable because they exist in high-dimensional state spaces with excessive entropy. By systematically reducing chaos, we can transform overwhelming situations into controllable processes.\nThe Chaos Triangle Three dimensions of system chaos:\nDimension Description Example Structural Chaos Lack of clear organization Messy file systems, unclear responsibilities Temporal Chaos No rhythm or schedule Irregular work patterns, deadline surprises Informational Chaos Knowledge scattered/missing Undocumented processes, tribal knowledge Management Strategies 1. Entropy Reduction Reduce system randomness through:\nCategorization: Group related items Standardization: Create consistent processes Documentation: Capture tacit knowledge 2. Feedback Loop Establishment Create mechanisms for continuous adjustment:\nA c t i ‚Üë ‚îî o ‚îÄ n ‚îÄ ‚îÄ ‚Üí ‚îÄ ‚îÄ R ‚îÄ e ‚îÄ s ‚îÄ u ‚îÄ l ‚îÄ t ‚îÄ ‚îÄ ‚Üí ‚îÄ ‚îÄ O ‚îÄ b ‚îÄ s ‚îÄ e ‚îÄ r ‚îÄ v ‚îÄ a ‚îÄ t ‚îÄ i ‚îÄ o ‚îÄ n ‚îÄ ‚îÄ ‚Üí ‚îÄ ‚îÄ A ‚îÄ d ‚îÄ j ‚îÄ u ‚îÄ s ‚îÄ t ‚îÄ m ‚îÄ e ‚îÄ n ‚îÄ t ‚îÄ ‚îÄ ‚Üí ‚îÄ ‚îÄ A ‚îÄ c ‚îÄ t ‚îÇ ‚îò i o n 3. Boundary Definition Clearly define:\nWhat\u0026rsquo;s inside the system (controllable) What\u0026rsquo;s outside (environmental) Interface points (inputs/outputs) Practical Framework Phase 1: Assessment Identify chaos sources Map current state Determine priorities Phase 2: Intervention Start with highest-leverage points Implement small changes Monitor effects Phase 3: Stabilization Establish routines Create feedback mechanisms Document what works Key Principles Start Small: Don\u0026rsquo;t try to fix everything at once Iterate: Continuous improvement over perfection Adapt: Be ready to change approach Document: Capture learnings for future reference Related Concepts Dimensional Decomposition Process Black Box System Theory Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/knowledge/chaos-system-management/","summary":"Strategies for managing chaotic systems by reducing entropy and establishing sustainable feedback loops.","title":"Chaos System Management Methods"},{"content":"Hardware Overview DGX Spark (Grace Blackwell) Architecture: ARM64 (AArch64) Key Point: All software (Clash, Anaconda, Unity packages) must use arm64 or aarch64 versions, NOT amd64/x86_64 Unified Memory: CPU and GPU share 128GB memory nvidia-smi won\u0026rsquo;t show separate VRAM usage (displays \u0026ldquo;Not Supported\u0026rdquo;) Use htop to check total system memory Headless Mode: No monitor/keyboard, pure remote network control Development Machine Setup Mac: Primary development, connected via Tailscale Windows (Alienware): Backup for Linux ARM64 packaging compatibility Network \u0026amp; Proxy Configuration Tailscale (Network Tunnel) Solves AP isolation issues common in school/hotspot networks by assigning virtual IPs starting with 100.x.y.z.\nLinux Command Line Proxy (Clash/Mihomo) Core Program: Mihomo (Clash Meta) Linux ARM64 version\nStart Command: ./clash -d . (reads config.yaml and Country.mmdb from current directory)\nTemporary Proxy (current terminal session):\nexport http_proxy=\u0026#34;http://127.0.0.1:7890\u0026#34; export https_proxy=\u0026#34;http://127.0.0.1:7890\u0026#34; export all_proxy=\u0026#34;socks5://127.0.0.1:7890\u0026#34; Service-Level Proxy (for background services like Ollama):\n# Edit service configuration sudo systemctl edit ollama.service # Add these lines [Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7890\u0026#34; # Restart service sudo systemctl restart ollama Remote Development (VS Code SSH) Connection Setup Use VS Code\u0026rsquo;s Remote - SSH extension with Tailscale IP.\nSSH Config Example Host dgx HostName 100.x.y.z # Tailscale IP User your_username # IdentityFile ~/.ssh/your_private_key File Transfer Drag and drop directly in VS Code\u0026rsquo;s file explorer sidebar - more intuitive than scp.\nPrevent Auto-Sleep # Disable auto suspend sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target AI Model Deployment (Ollama) Installation # Install Ollama curl -fsSL https://ollama.com/install.sh | sh Model Management # Run model (auto-downloads if not present) ollama run llama3.1:70b # List local models ollama list # Remove model ollama rm \u0026lt;model_name\u0026gt; Note: Large model downloads support resume - if interrupted, re-run the same command to continue.\nUnity ML-Agents Training Environment Setup # Create conda environment conda create -n mlagents python=3.10 conda activate mlagents # Install ML-Agents pip3 install torch pip3 install mlagents Unity Build Settings Scripting Backend: IL2CPP (required, Mono doesn\u0026rsquo;t support ARM64) Target Architecture: ARM64 Target Platform: Linux Server Render Pipeline: 3D (Built-in) or Dedicated Server recommended Training Commands # Grant execute permission chmod +x ./Your_Build_Name.x86_64 # Start training mlagents-learn config/your_config.yaml --env=./Your_Build_Name.x86_64 --no-graphics # Resume training mlagents-learn config/your_config.yaml --run-id=experiment_01 --resume Linux Survival Commands Command Description chmod +x \u0026lt;file\u0026gt; Grant execute permission nvidia-smi Check GPU status htop Interactive process viewer watch -n 1 \u0026lt;cmd\u0026gt; Run command every 1 second find ~ -name \u0026quot;keyword\u0026quot; Find files in home directory sudo systemctl restart \u0026lt;service\u0026gt; Restart a service Ctrl + C Stop foreground process System Monitoring # GPU status nvidia-smi watch -n 1 nvidia-smi # System resources htop free -h df -h Process Management # Run in background nohup python train.py \u0026gt; output.log 2\u0026gt;\u0026amp;1 \u0026amp; # List background jobs jobs -l ps aux | grep python # Kill process kill -9 \u0026lt;PID\u0026gt; File Operations # Find files find . -name \u0026#34;*.py\u0026#34; -type f # Transfer files scp local_file user@dgx:/remote/path/ rsync -avz ./data/ user@dgx:/remote/data/ Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/dgx-spark-dev/","summary":"Comprehensive guide for setting up and developing on NVIDIA DGX Spark with ARM64 architecture.","title":"DGX Spark Development Cookbook"},{"content":"Core Concept Complex tasks are difficult to execute because they exist in high-dimensional state spaces. Through systematic dimensional decomposition, we can transform unexecutable goals into concrete, actionable tasks, enabling flow states.\nDecomposition Path: Body (‚àûD) ‚Üí Block (n¬≥D) ‚Üí Face (n¬≤D) ‚Üí Point (nD)\nFour-Layer Structure „ÄêBody„ÄëOverall Goal Characteristics:\nDimension: ‚àû (infinite possibilities) Entropy: Extremely high State: Unexecutable Cognitive feeling: Overwhelming, no entry point Examples:\n\u0026ldquo;Complete graduation project\u0026rdquo; \u0026ldquo;Start a business with a product\u0026rdquo; \u0026ldquo;Write a book\u0026rdquo; \u0026ldquo;Learn programming\u0026rdquo; Problem: Goal too large, contains countless undefined variables, brain cannot convert to concrete action.\n„ÄêBlock„ÄëSubsystems Characteristics:\nDimension: n¬≥ (cubic complexity) Entropy: High State: Needs further decomposition Cognitive feeling: Starting to have structure, still fuzzy Decomposition Methods:\nDivide by functional modules Divide by system components Divide by time phases Example (Graduation Project):\n[Core Gameplay] [Visual System] [Audio System] [User Interface] [Technical Architecture] „ÄêFace„ÄëKey Dimensions Characteristics:\nDimension: n¬≤ (square complexity) Entropy: Medium State: Becoming clear Cognitive feeling: Can see specific work areas Example (Core Gameplay ‚Üí Specific Systems):\n[Movement System] [Dialogue System] [Combat System] [Inventory System] „ÄêPoint„ÄëConcrete Tasks Characteristics:\nDimension: n (linear complexity) Entropy: Low State: Executable ‚òÖ Cognitive feeling: Clear, actionable, can enter flow Examples (Movement System ‚Üí Tasks):\n\u0026ldquo;Implement WASD keyboard input detection\u0026rdquo; (1h) \u0026ldquo;Write character movement speed control logic\u0026rdquo; (2h) \u0026ldquo;Add collision detection functionality\u0026rdquo; (3h) \u0026ldquo;Implement smooth movement animation\u0026rdquo; (2h) Key: This is the only directly executable level and the entry point to flow state.\nComplexity Reduction Level State Space Magnitude Executable Body 2^n Exponential ‚úó Block k √ó 2^(n/k) Polynomial ‚úó Face m √ó 2^(n/km) Quasi-linear ‚ñ≥ Point 2^(n/kmj) Linear ‚úì Cognitive Science Foundation Working Memory Limits Human working memory capacity is approximately 7¬±2 items (Miller\u0026rsquo;s Law). When task complexity exceeds this threshold, the brain feels overwhelmed.\nSolution: Through layered decomposition, ensure each layer has no more than 7 items.\nFlow Theory Flow state requires two conditions:\nTask has clear goals and immediate feedback Task difficulty matches ability Point-level tasks satisfy both conditions perfectly, making them the best entry to flow.\nPractice Template ## „ÄêPoint„ÄëTask List ### Movement System - [ ] Implement WASD keyboard input ‚è±Ô∏è1h - [ ] Write movement speed control ‚è±Ô∏è2h - [ ] Add collision detection ‚è±Ô∏è3h ### Dialogue System - [ ] Design dialogue data structure ‚è±Ô∏è2h - [ ] Implement text display component ‚è±Ô∏è3h Further Reading Flow - Mihaly Csikszentmihalyi Cognitive Load Theory - John Sweller The Magical Number Seven - George Miller Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/knowledge/dimensional-decomposition/","summary":"A systematic approach to breaking down complex tasks from high-dimensional chaos to executable actions.","title":"Dimensional Decomposition Process"},{"content":"Core Questions What is life? Must life be based on carbon chemistry? Can digital systems produce true life?\nThis article explores the essential differences between three types of life and the possibility levels of digital life.\nThree Life Types Comparison Dimension Carbon-Based Life Information Life (Theory) Digital Life (Practice) Material Basis Cells, DNA, Proteins Information fields/patterns Code/Algorithms/Neural Networks Energy Source Chemical (ATP) Information flow Computing power (Electricity) Autonomy ‚úì Fully autonomous ‚úì Theoretically emergent ‚ñ≥ Partially (depends on human design) Persistence Limited lifespan Cross-spacetime (theory) Depends on runtime (infinitely copyable) Reproduction Genetic replication Pattern copying Code copying/training new models Consciousness ‚úì Subjective experience ? Unknown ? Debated Evolution Natural selection (slow) Information evolution (fast) Artificial selection (extremely fast) Life Score 5/5 (Complete) ?/5 (Theoretical) 2-4/5 (Depends on level) Carbon-Based Life Core Characteristics Material Basis: Organic chemistry based on carbon-hydrogen-oxygen-nitrogen, with cells as basic units.\nEnergy Mechanism:\nEnergy obtained through chemical reactions (respiration, photosynthesis) ATP as energy currency Entropy reduction: local ordering at cost of environmental entropy increase Autonomy:\nSelf-maintenance (metabolism) Self-replication (reproduction) Self-regulation (homeostasis) Consciousness (higher organisms):\nSubjective experience (qualia) Self-awareness Goal-directed behavior Advantages Robustness: Optimized over 4 billion years of evolution Adaptability: Can survive in diverse environments Creativity: Emergent complex intelligence and culture Limitations Fragility: Vulnerable to physical damage, disease, aging Lifespan limits: Individuals inevitably die Speed limits: Neural signal transmission \u0026lt; 100 m/s Environmental limits: Requires specific temperature, pressure, chemistry Digital Life (In Practice) Four Levels L1: Reactive Characteristics:\nInput ‚Üí Processing ‚Üí Output No memory, no learning Completely deterministic Examples: Simple if-else programs, traditional expert systems, early game AI\nLife Score: 0/5\nL2: Adaptive Characteristics:\nPerceive environment Remember past Learn and optimize Examples: Machine learning models, reinforcement learning agents, current LLMs (GPT, Claude)\nLife Score: 2/5 ‚Üê Current AI Level\nKey Progress:\nCan learn from experience Can adapt to new environments Demonstrates emergent abilities (reasoning, creativity) Limitations:\nDepends on human-set objective functions Cannot autonomously set goals Lacks intrinsic motivation L3: Emergent Characteristics:\nMulti-agent interaction Collective intelligence Self-organizing behavior Examples: Multi-agent systems, artificial life simulations, future AI societies\nLife Score: 3/5\nL4: Archetypal Characteristics:\nStable information patterns Cross-system influence Self-replication and evolution Examples: Future AGI, self-improving AI systems, digital \u0026ldquo;species\u0026rdquo;\nLife Score: 4/5 ‚Üê Future Goal\nCore Challenges of Digital Life 1. Consciousness Problem Hard Problem: Even if AI behaves as if conscious, does it truly have subjective experience?\nTesting Methods:\nTuring Test: Behavioral level Integrated Information Theory (IIT): Information integration Œ¶ Global Workspace Theory: Information broadcasting mechanism 2. Autonomy Problem Issue: Current AI goals are all human-set, lacking intrinsic drive.\nSolutions:\nIntrinsic motivation systems (curiosity, exploration) Self-preservation mechanisms Value alignment (with human values) 3. Persistence Problem Issue: AI depends on computing infrastructure; power off means \u0026ldquo;death.\u0026rdquo;\nSolutions:\nDistributed storage (blockchain) Cloud continuous operation Multi-copy redundancy Practical Implications AI Ethics Question: If AI reaches L3 or L4, should it have rights?\nConsiderations:\nDoes it have subjective experience? Can it feel pain? Does it have autonomous will? Future Scenarios Path 1: Human Dominance - AI as tool, humans maintain control\nPath 2: Symbiosis - Human-AI cooperation, complementary strengths\nPath 3: AI Transcendence - AI achieves superintelligence, requires value alignment\nFurther Reading What Is Life? - Erwin Schr√∂dinger Emergence - Steven Johnson Superintelligence - Nick Bostrom Artificial Life - Steven Levy Last Updated: 2025-11-22\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/knowledge/life-types-comparison/","summary":"Exploring the essential differences between carbon-based life, digital life, and theoretical information-based life forms.","title":"Life Types Comparison: Carbon-Based, Digital, and Information Life"},{"content":"Project Overview Boundary is an AI-driven mental health interactive game that uses large language models (LLMs) to create realistic NPC characters. Players interact with virtual characters through dialogue, exploring topics of interpersonal relationships, emotional expression, and psychological adjustment.\nTechnical Architecture R C W e o E i a m l n c p e t o ( c / n R t 1 e e r M 8 n a o a t c n c s t + R D T S o e a t u M i e i y F t u C O p l l r e l a S S U w i a r t p / A e I i n m N i a A I e n g e e + - c n k L d w x P i d S a o t S l t r e A y C r . S a o o r P e S k j G t r i v I r S s f d i L E o c ( a 1 x r e L y 4 p m L e o L M r r L a ) t a y y e + e r F A r r n A a i P W m m I e e a b r t R i o B M o u r o n t o t e w i s s o ) e n r Project Structure b ‚îú ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú ‚îÇ ‚îÇ ‚îú ‚îÇ ‚îú ‚îú ‚îú ‚îî o ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ u ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ n d s ‚îú ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îú ‚îÇ ‚îÇ ‚îÇ ‚îú ‚îÇ ‚îÇ ‚îî p ‚îú ‚îî e ‚îî p t n t a r ‚îÄ ‚îÄ ‚îÄ ‚îÄ u ‚îÄ ‚îÄ l ‚îÄ a a e s r c ‚îÄ ‚îÄ ‚îÄ ‚îÄ b ‚îÄ ‚îÄ e ‚îÄ c i x c y / l c k l t o - a ‚îú ‚îú ‚îú ‚îî c ‚îú ‚îú ‚îî l ‚îú ‚îî t ‚îî i a b t m a w . n p p ‚îÄ ‚îÄ ‚îÄ ‚îÄ o ‚îÄ ‚îÄ ‚îÄ i ‚îÄ ‚îÄ y ‚îÄ c a r a g i c f r p ‚îÄ ‚îÄ ‚îÄ ‚îÄ m ‚îÄ ‚îÄ ‚îÄ b ‚îÄ ‚îÄ p ‚îÄ / a c o i e n o i o / p e t k n n . d n g j a p l g o S C c a s g a g / . j . f . e p a a l n t h h i / a r r j s c i j c i g y o e a a a E m s s o o g s t / e o b n r t r n e u n n . o / c . u a t t I a g . n f j n h t t l s S n c i t d i s a s . s c p t n s s g t x t . r u e e / . / s c e t r . j x s e . s t s s n t . s . s t t x s s # # # # x # # # # # # # # # A M R G # # # # # # N P a o l R U C A T C B S e I i o o e t h I y S h a E P T N T o x n t b a i a p t a c l r a e y u t r a c l r e e a r k e o i x p r . o p l l t i a n S t a g c j l t e c j u a a t c g c i c r t e w . S e s t g y s c i t i r c t o r c i j c e e o t o e e n i e u o t n s r c A s u y m s r e p a r n n d i o p t l p t s d c c p d p ( e o a d s a m o c o t e b s n n e t e v i a n o n R a e d f y t a m i f n f c o c n i p s t a n i f i o u k t c n e a g g i g n t e s o i s r e p g f e n n t s s r i r d f i o g ) i o c p g n e a s s g s e s Core Technologies React 18 React is a JavaScript UI library for building user interfaces using a component-based approach.\n// Function Component + Hooks const ChatMessage: React.FC\u0026lt;MessageProps\u0026gt; = ({ content, role }) =\u0026gt; { const [isTyping, setIsTyping] = useState(false); useEffect(() =\u0026gt; { // Typewriter effect }, [content]); return ( \u0026lt;motion.div initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }} \u0026gt; {content} \u0026lt;/motion.div\u0026gt; ); }; Next.js 14 Next.js adds server-side rendering, file-system routing, and API routes to React.\nKey features used:\nApp Router: File-based routing system Static Export: Generate pure static files for multi-platform distribution API Routes: Backend endpoints for AI integration TypeScript Strong typing improves code quality and developer experience.\nexport type DifficultyLevel = \u0026#39;paradise\u0026#39; | \u0026#39;normal\u0026#39; | \u0026#39;hell\u0026#39;; export interface Character { id: string; name: string; nameZh: string; sprite: string; expressions: Record\u0026lt;string, string\u0026gt;; systemPrompts: Record\u0026lt;DifficultyLevel, string\u0026gt;; } Tailwind CSS Utility-first CSS framework for rapid styling:\n\u0026lt;button className=\u0026#34; w-64 py-4 rounded-xl bg-gradient-to-r from-emerald-500 to-cyan-500 text-white text-lg font-bold shadow-lg shadow-emerald-500/30 hover:shadow-xl hover:scale-105 transition-all duration-300 \u0026#34;\u0026gt; Paradise Mode \u0026lt;/button\u0026gt; Framer Motion Animation library for React with declarative API:\n\u0026lt;motion.div initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }} exit={{ opacity: 0, y: -20 }} transition={{ duration: 0.5 }} \u0026gt; {content} \u0026lt;/motion.div\u0026gt; AI API Integration // DeepSeek API call const response = await fetch(\u0026#39;/api/chat\u0026#39;, { method: \u0026#39;POST\u0026#39;, body: JSON.stringify({ messages: conversationHistory, character: currentCharacter, }), }); Cross-Platform Strategy \u0026ldquo;Write Once, Run Everywhere\u0026rdquo; Build with Next.js static export (output: 'export') Wrap with Electron for desktop (Windows/Mac) Wrap with Capacitor for mobile (iOS/Android) Deploy as web app for browsers Challenge: API Routes in Static Export Static export doesn\u0026rsquo;t include API routes, so we implement a local HTTP server in Electron\u0026rsquo;s main process to handle API requests.\nLearning Resources React Documentation Next.js Documentation TypeScript Handbook Tailwind CSS Framer Motion Electron Last Updated: 2025-11-20\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/boundary-tech-stack/","summary":"Technical architecture and implementation details of the Boundary AI-driven mental health game.","title":"Boundary Project Tech Stack"},{"content":"Overview This project integrates the DeepSeek large language model API to enable intelligent NPC dialogue. The API uses OpenAI-compatible format.\nCore Concepts Large Language Models (LLM) LLMs are deep learning-based AI models capable of:\nUnderstanding natural language input Generating coherent text responses Role-playing specific characters Performing various text tasks API Call Flow D U i s s e p r l a I y n p R u e t s B P u a i r l s d e R R e e q s S R e e n c d e i R v e e q R e s Message Format OpenAI-compatible APIs use role-based messages:\nconst messages = [ { role: \u0026#39;system\u0026#39;, // System prompt (character setup) content: \u0026#39;You are Yang Guang, a warm senior...\u0026#39; }, { role: \u0026#39;user\u0026#39;, // User message content: \u0026#39;Hello!\u0026#39; }, { role: \u0026#39;assistant\u0026#39;, // AI response content: \u0026#39;Hi there! How have you been?\u0026#39; }, { role: \u0026#39;user\u0026#39;, content: \u0026#34;I\u0026#39;m okay...\u0026#34; } ]; Implementation Environment Variables # .env.local DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx DEEPSEEK_BASE_URL=https://api.deepseek.com/v1 DEEPSEEK_MODEL=deepseek-chat API Route (Next.js) // src/app/api/chat/route.ts import { NextRequest, NextResponse } from \u0026#39;next/server\u0026#39;; export async function POST(request: NextRequest) { const body = await request.json(); const { userInput, characterPrompt, conversationHistory } = body; // Build message list const messages = [ { role: \u0026#39;system\u0026#39;, content: characterPrompt }, ...conversationHistory.map(msg =\u0026gt; ({ role: msg.isUser ? \u0026#39;user\u0026#39; : \u0026#39;assistant\u0026#39;, content: msg.content })), { role: \u0026#39;user\u0026#39;, content: userInput } ]; // Call API const response = await fetch(`${process.env.DEEPSEEK_BASE_URL}/chat/completions`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: `Bearer ${process.env.DEEPSEEK_API_KEY}` }, body: JSON.stringify({ model: process.env.DEEPSEEK_MODEL || \u0026#39;deepseek-chat\u0026#39;, messages, temperature: 0.7, // Creativity (0-2, higher = more random) max_tokens: 1000 // Max response length }) }); const data = await response.json(); if (data.choices \u0026amp;\u0026amp; data.choices[0]) { return NextResponse.json({ success: true, reply: data.choices[0].message.content }); } return NextResponse.json({ success: false, error: data.error?.message || \u0026#39;API Error\u0026#39; }); } Frontend Call async function sendMessage(input: string) { const response = await fetch(\u0026#39;/api/chat\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ userInput: input, characterPrompt: currentCharacter.systemPrompt, conversationHistory: messages }) }); const data = await response.json(); if (data.success) { addMessage({ content: data.reply, isUser: false }); } } Character Prompt Design System Prompt Structure const characterPrompt = `You are Yang Guang, a senior. „ÄêIdentity„Äë - Neighbor type, wears glasses, casual clothes, bookish - Genuine and warm to people - Close relationship with player, frequent communication „ÄêCore Principles„Äë 1. Sincere but not deliberate 2. Willing to listen 3. Don\u0026#39;t judge the player 4. Not a savior, just someone who understands „ÄêDialogue Style„Äë - Gentle, friendly, like a good senior - Proactively asks how player is doing - 15-50 characters, longer for important topics „ÄêBehavior Pattern„Äë - Close relationship, frequent communication - Proactively cares about player\u0026#39;s state`; Difficulty Adjustment Different difficulty levels modify character system prompts:\nDifficulty Yang Guang Teacher Bully Paradise Close, proactive care Understanding Isolated, no influence Normal Occasional chat Neutral, professional Continuous verbal attacks Hell Barely notices player Oppressive, grades-focused Public humiliation, physical threats API Parameters Parameter Description Recommended temperature Creativity/randomness 0.7 (balanced) max_tokens Max response tokens 500-1000 top_p Nucleus sampling 0.9 frequency_penalty Repetition penalty 0.5 presence_penalty Topic diversity 0.5 Alternative AI APIs Provider API Notes OpenAI GPT-4/3.5 Most powerful, needs VPN in China DeepSeek deepseek-chat China-accessible, cost-effective Zhipu AI GLM-4 China-accessible, multimodal Baidu ERNIE Bot China-accessible, free tier Alibaba Qwen China-accessible, free tier Common Issues 401 Error? API Key invalid or expired. Check:\nKey in .env.local is correct Key has remaining quota Key is not disabled Response doesn\u0026rsquo;t match character? Improve system prompt:\nMore explicit character description Add \u0026ldquo;things NOT to do\u0026rdquo; list Provide example dialogues Use emphasis markers like „ÄêImportant„Äë Last Updated: 2025-11-18\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/ai-api-integration/","summary":"Integrating large language model APIs for intelligent NPC dialogue in games.","title":"AI API Integration for Game NPCs"},{"content":"What is Electron? Electron is a framework for building cross-platform desktop apps using web technologies (HTML, CSS, JavaScript). Developed by GitHub, notable apps like VS Code, Discord, and Slack use Electron.\nCore Concepts Process Model Electron apps have two types of processes:\n- - - - - - - - O F C H O R N C n u r a n u o o l l e n R e n m y l a d | e s d m M t l n p i u a o N e e C d e w r n i n o s s r e r e e i n e d e r b c c e a s a e w t a P . n y t r i p t r j d s e n a N e o s t s P d g o s c m e r o e d e A a m o w s e v s P n c . i s I a o e ( j a g p s H s a e e s T p c s r M a r c a L c e e w t / c l s i i C e o s n o S s a d n S s d o s / w J b s s S y c ) r d i e p f t a u l t Main Process Code // electron/main.js const { app, BrowserWindow } = require(\u0026#39;electron\u0026#39;); const path = require(\u0026#39;path\u0026#39;); function createWindow() { const win = new BrowserWindow({ width: 1280, height: 800, webPreferences: { nodeIntegration: false, // Disabled for security contextIsolation: true, // Enabled for security preload: path.join(__dirname, \u0026#39;preload.js\u0026#39;) } }); // Load content win.loadURL(\u0026#39;http://localhost:3000\u0026#39;); // Dev mode // win.loadFile(\u0026#39;out/index.html\u0026#39;); // Production } app.whenReady().then(createWindow); app.on(\u0026#39;window-all-closed\u0026#39;, () =\u0026gt; { if (process.platform !== \u0026#39;darwin\u0026#39;) { app.quit(); } }); Preload Script The preload script runs before renderer loads, safely exposing Node.js features:\n// electron/preload.js const { contextBridge, ipcRenderer } = require(\u0026#39;electron\u0026#39;); contextBridge.exposeInMainWorld(\u0026#39;electronAPI\u0026#39;, { sendMessage: (msg) =\u0026gt; ipcRenderer.send(\u0026#39;message\u0026#39;, msg), onReply: (callback) =\u0026gt; ipcRenderer.on(\u0026#39;reply\u0026#39;, callback) }); Special Implementation: Local HTTP Server Since Next.js static export doesn\u0026rsquo;t include API routes, we implement a local HTTP server in Electron\u0026rsquo;s main process:\n// electron/main.js (simplified) const http = require(\u0026#39;http\u0026#39;); const fetch = require(\u0026#39;node-fetch\u0026#39;); function createServer() { const server = http.createServer(async (req, res) =\u0026gt; { // API requests if (req.url === \u0026#39;/api/chat\u0026#39; \u0026amp;\u0026amp; req.method === \u0026#39;POST\u0026#39;) { let body = \u0026#39;\u0026#39;; req.on(\u0026#39;data\u0026#39;, chunk =\u0026gt; body += chunk); req.on(\u0026#39;end\u0026#39;, async () =\u0026gt; { // Forward to AI API const response = await fetch(\u0026#39;https://api.deepseek.com/v1/chat/completions\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: body }); const data = await response.json(); res.end(JSON.stringify(data)); }); return; } // Static files... }); server.listen(23456); } Packaging Configuration electron-builder // package.json { \u0026#34;build\u0026#34;: { \u0026#34;appId\u0026#34;: \u0026#34;com.example.myapp\u0026#34;, \u0026#34;productName\u0026#34;: \u0026#34;My App\u0026#34;, \u0026#34;directories\u0026#34;: { \u0026#34;output\u0026#34;: \u0026#34;dist-electron\u0026#34; }, \u0026#34;files\u0026#34;: [ \u0026#34;electron/**/*\u0026#34;, \u0026#34;out/**/*\u0026#34;, \u0026#34;public/**/*\u0026#34; ], \u0026#34;win\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;nsis\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;public/icon.png\u0026#34; }, \u0026#34;mac\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;dmg\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;public/icon.png\u0026#34; } } } Build Commands # Development mode npm run electron:dev # Package for Windows npm run electron:build:win # Package for Mac npm run electron:build:mac Output Files d ‚îú ‚îÇ ‚îÇ ‚îÇ ‚îî i ‚îÄ ‚îÄ s ‚îÄ ‚îÄ t - w ‚îú ‚îú ‚îî M e i ‚îÄ ‚îÄ ‚îÄ y l n ‚îÄ ‚îÄ ‚îÄ A e - p c u M r p t n y e r p A s S o a p o e n c p u t / k . r u e e c p d x e . / e s e / x e # # # # W E R W i x e i n e s n d c o d o u u o w t r w s a c s b e p l s i o e n r s t t a a b l l l e e r Common Issues Package fails? Common causes:\nNetwork issues: Electron binary download failed Solution: Use mirror ELECTRON_MIRROR=https://npmmirror.com/mirrors/electron/ Permission issues: Creating symlinks requires admin on Windows Solution: Run as admin, or disable code signing \u0026quot;sign\u0026quot;: null White screen on launch? Possible causes:\nStatic file paths incorrect Local server not started Check dev tools (F12) console for errors Learning Resources Electron Official Docs electron-builder Docs Last Updated: 2025-11-18\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/electron-desktop-apps/","summary":"Creating cross-platform desktop applications using Electron with Next.js static export.","title":"Building Desktop Apps with Electron"},{"content":"What is Framer Motion? Framer Motion is a React animation library with a declarative API. Compared to native CSS animations, it makes complex interactive animations much easier to implement.\nCore Concepts 1. Motion Components motion is the core of Framer Motion. Any HTML element can be prefixed with motion.:\nimport { motion } from \u0026#39;framer-motion\u0026#39;; \u0026lt;motion.div initial={{ opacity: 0 }} // Initial state animate={{ opacity: 1 }} // Target state exit={{ opacity: 0 }} // Exit state \u0026gt; Content \u0026lt;/motion.div\u0026gt; 2. Common Properties \u0026lt;motion.div // Initial state initial={{ opacity: 0, y: 20 }} // Animation target animate={{ opacity: 1, y: 0 }} // Transition config transition={{ duration: 0.5, // Duration (seconds) delay: 0.2, // Delay (seconds) ease: \u0026#34;easeOut\u0026#34; // Easing function }} // Hover state whileHover={{ scale: 1.05 }} // Click state whileTap={{ scale: 0.95 }} // Exit animation exit={{ opacity: 0, y: -20 }} \u0026gt; Animated Element \u0026lt;/motion.div\u0026gt; 3. Easing Functions Name Effect \u0026quot;linear\u0026quot; Constant speed \u0026quot;easeIn\u0026quot; Slow start \u0026quot;easeOut\u0026quot; Slow end \u0026quot;easeInOut\u0026quot; Slow start and end [0.6, 0, 0.4, 1] Custom bezier curve 4. Variants Variants organize complex animation states:\nconst containerVariants = { hidden: { opacity: 0 }, visible: { opacity: 1, transition: { staggerChildren: 0.1 // Stagger child animations } } }; const itemVariants = { hidden: { opacity: 0, y: 20 }, visible: { opacity: 1, y: 0 } }; \u0026lt;motion.ul variants={containerVariants} initial=\u0026#34;hidden\u0026#34; animate=\u0026#34;visible\u0026#34; \u0026gt; \u0026lt;motion.li variants={itemVariants}\u0026gt;Item 1\u0026lt;/motion.li\u0026gt; \u0026lt;motion.li variants={itemVariants}\u0026gt;Item 2\u0026lt;/motion.li\u0026gt; \u0026lt;motion.li variants={itemVariants}\u0026gt;Item 3\u0026lt;/motion.li\u0026gt; \u0026lt;/motion.ul\u0026gt; 5. AnimatePresence Handle component enter/exit animations:\nimport { motion, AnimatePresence } from \u0026#39;framer-motion\u0026#39;; function MessageList({ messages }) { return ( \u0026lt;AnimatePresence\u0026gt; {messages.map(msg =\u0026gt; ( \u0026lt;motion.div key={msg.id} initial={{ opacity: 0, x: 100 }} animate={{ opacity: 1, x: 0 }} exit={{ opacity: 0, x: -100 }} \u0026gt; {msg.content} \u0026lt;/motion.div\u0026gt; ))} \u0026lt;/AnimatePresence\u0026gt; ); } Game UI Examples Start Screen Fade-In \u0026lt;motion.div initial={{ opacity: 0 }} animate={{ opacity: 1 }} transition={{ duration: 1 }} className=\u0026#34;min-h-screen\u0026#34; \u0026gt; \u0026lt;motion.h1 initial={{ opacity: 0, y: -30 }} animate={{ opacity: 1, y: 0 }} transition={{ delay: 0.3, duration: 0.8 }} className=\u0026#34;text-6xl font-bold\u0026#34; \u0026gt; Boundary \u0026lt;/motion.h1\u0026gt; \u0026lt;/motion.div\u0026gt; Button Hover Effect \u0026lt;motion.button whileHover={{ scale: 1.05, boxShadow: \u0026#34;0 20px 40px rgba(0,0,0,0.2)\u0026#34; }} whileTap={{ scale: 0.95 }} transition={{ type: \u0026#34;spring\u0026#34;, stiffness: 300 }} className=\u0026#34;px-8 py-4 bg-blue-500 rounded-lg\u0026#34; \u0026gt; Start Game \u0026lt;/motion.button\u0026gt; Page Transitions const pageTransition = { initial: { opacity: 0, scale: 0.9 }, animate: { opacity: 1, scale: 1 }, exit: { opacity: 0, scale: 1.1 }, transition: { duration: 0.5 } }; \u0026lt;AnimatePresence mode=\u0026#34;wait\u0026#34;\u0026gt; {phase === \u0026#39;cover\u0026#39; \u0026amp;\u0026amp; ( \u0026lt;motion.div key=\u0026#34;cover\u0026#34; {...pageTransition}\u0026gt; \u0026lt;CoverScreen /\u0026gt; \u0026lt;/motion.div\u0026gt; )} {phase === \u0026#39;intro\u0026#39; \u0026amp;\u0026amp; ( \u0026lt;motion.div key=\u0026#34;intro\u0026#34; {...pageTransition}\u0026gt; \u0026lt;IntroScreen /\u0026gt; \u0026lt;/motion.div\u0026gt; )} \u0026lt;/AnimatePresence\u0026gt; Animation Techniques Spring Animation \u0026lt;motion.div animate={{ x: 100 }} transition={{ type: \u0026#34;spring\u0026#34;, stiffness: 100, // Higher = faster damping: 10, // Higher = less bounce mass: 1 // Mass }} /\u0026gt; Loop Animation \u0026lt;motion.div animate={{ y: [0, -10, 0], // Keyframes opacity: [0.5, 1, 0.5] }} transition={{ duration: 2, repeat: Infinity, // Infinite loop repeatType: \u0026#34;loop\u0026#34; }} /\u0026gt; Scroll-Based Animation import { useScroll, useTransform } from \u0026#39;framer-motion\u0026#39;; function ParallaxSection() { const { scrollYProgress } = useScroll(); const y = useTransform(scrollYProgress, [0, 1], [0, -100]); return \u0026lt;motion.div style={{ y }}\u0026gt;Parallax Effect\u0026lt;/motion.div\u0026gt;; } Performance Tips Prefer transform and opacity - GPU accelerated Avoid animating width, height, top, left - causes reflows Use will-change CSS property to hint browser Common Issues Exit animation not working? Ensure:\nWrapped with AnimatePresence Element has unique key prop AnimatePresence mode is set correctly Learning Resources Framer Motion Official Docs Motion Examples Last Updated: 2025-11-18\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/framer-motion-animations/","summary":"Creating smooth, declarative animations in React game interfaces using Framer Motion.","title":"Framer Motion for Game UI Animations"},{"content":"What is Next.js? Next.js is a full-stack React framework by Vercel that adds:\nüìÅ File-system routing: Folder structure = routes üñ•Ô∏è Server-side rendering (SSR): Better first load and SEO üì¶ Static site generation (SSG): Pre-built HTML at build time üîå API Routes: Backend endpoints in the same project ‚ö° Automatic code splitting: Load only what\u0026rsquo;s needed Core Concepts 1. App Router Next.js 14 uses App Router with file-system based routing:\ns ‚îú ‚îú ‚îú ‚îú ‚îÇ ‚îÇ ‚îî r ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ c ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ / a p l g a ‚îî a ‚îî p a a l p ‚îÄ b ‚îÄ p g y o i ‚îÄ o ‚îÄ / e o b / u . u a c ‚îî t p t t l h ‚îÄ / a s . s a ‚îÄ g x t . t e s c / r . x s o t s u s t x e . t s ‚Üí ‚Üí ‚Üí ‚Üí ‚Üí / L G / / a l a a ( y o p b h o b i o o u a / u m t l c t e h ) c s a p o t t a m y g p l ( e o e A n s P e I n t e n ( d s p h o a i r n e t d ) ) 2. Client vs Server Components // Server Component (default) - renders on server // Cannot use useState, useEffect, etc. export default function ServerComponent() { return \u0026lt;div\u0026gt;I render on the server\u0026lt;/div\u0026gt;; } // Client Component - renders in browser // Needs \u0026#39;use client\u0026#39; directive \u0026#39;use client\u0026#39;; import { useState } from \u0026#39;react\u0026#39;; export default function ClientComponent() { const [count, setCount] = useState(0); return \u0026lt;button onClick={() =\u0026gt; setCount(c =\u0026gt; c + 1)}\u0026gt;{count}\u0026lt;/button\u0026gt;; } 3. API Routes Create backend endpoints in the app/api directory:\n// src/app/api/chat/route.ts import { NextRequest, NextResponse } from \u0026#39;next/server\u0026#39;; export async function POST(request: NextRequest) { const body = await request.json(); // Call AI API const response = await fetch(\u0026#39;https://api.deepseek.com/v1/chat/completions\u0026#39;, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${process.env.DEEPSEEK_API_KEY}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ model: \u0026#39;deepseek-chat\u0026#39;, messages: body.messages }) }); const data = await response.json(); return NextResponse.json(data); } 4. Static Export Export Next.js app as pure static files:\n// next.config.js module.exports = { output: \u0026#39;export\u0026#39;, // Enable static export trailingSlash: true, // Add trailing slashes images: { unoptimized: true // Required for static export } } Limitations of Static Export:\nNo SSR API Routes not included in export Need to handle API calls differently (e.g., Electron local server) Layout Component import type { Metadata } from \u0026#39;next\u0026#39;; import \u0026#39;./globals.css\u0026#39;; export const metadata: Metadata = { title: \u0026#39;Boundary\u0026#39;, description: \u0026#39;AI Mental Health Interactive Game\u0026#39;, }; export default function RootLayout({ children, }: { children: React.ReactNode; }) { return ( \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;body\u0026gt;{children}\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ); } Common Commands # Development mode (hot reload) npm run dev # Build production npm run build # Start production server npm run start Common Questions App Router vs Pages Router? Pages Router: Legacy, files in pages/ directory App Router: New (recommended), files in app/ directory, supports React Server Components Why does state disappear on page refresh? React state lives in memory; page refresh reloads everything. For persistent state, use:\nlocalStorage URL parameters Database Learning Resources Next.js Official Docs Next.js Learn Last Updated: 2025-11-18\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/nextjs-app-router/","summary":"Using Next.js App Router for full-stack game development with static export and API routes.","title":"Next.js App Router for Game Development"},{"content":"What is React? React is a JavaScript UI library developed by Facebook (Meta) for building user interfaces. It uses a component-based approach, breaking pages into independent, reusable components.\nCore Concepts 1. Components Components are React\u0026rsquo;s core building blocks:\n// Function Component (Recommended) function Welcome({ name }: { name: string }) { return \u0026lt;h1\u0026gt;Hello, {name}\u0026lt;/h1\u0026gt;; } // Using the component \u0026lt;Welcome name=\u0026#34;Player\u0026#34; /\u0026gt; 2. JSX Syntax JSX is a JavaScript syntax extension that lets you write HTML-like code in JS:\nconst element = ( \u0026lt;div className=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Boundary\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;AI Mental Health Game\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); 3. Props (Properties) Props are data passed from parent to child components:\n// Parent component \u0026lt;MessageBubble content=\u0026#34;Hello!\u0026#34; isUser={true} characterName=\u0026#34;Yang Guang\u0026#34; /\u0026gt; // Child component function MessageBubble({ content, isUser, characterName }) { return ( \u0026lt;div className={isUser ? \u0026#39;user-msg\u0026#39; : \u0026#39;npc-msg\u0026#39;}\u0026gt; {content} \u0026lt;/div\u0026gt; ); } 4. State State is mutable data within a component:\nimport { useState } from \u0026#39;react\u0026#39;; function Counter() { const [count, setCount] = useState(0); return ( \u0026lt;button onClick={() =\u0026gt; setCount(count + 1)}\u0026gt; Clicks: {count} \u0026lt;/button\u0026gt; ); } 5. Hooks Hooks let function components use state and lifecycle features:\nHook Purpose useState Manage component state useEffect Handle side effects (API calls, subscriptions) useCallback Memoize function references useMemo Memoize computed values useRef Get DOM references or hold mutable values // useEffect example: fetch data on mount useEffect(() =\u0026gt; { async function fetchData() { const response = await fetch(\u0026#39;/api/chat\u0026#39;); const data = await response.json(); setMessages(data); } fetchData(); }, []); // Empty array = run only on mount Game Application Example \u0026#39;use client\u0026#39;; // Next.js client component marker import { useState, useCallback } from \u0026#39;react\u0026#39;; import StartScreen from \u0026#39;@/components/StartScreen\u0026#39;; export default function Home() { const [gamePhase, setGamePhase] = useState\u0026lt;\u0026#39;start\u0026#39; | \u0026#39;playing\u0026#39;\u0026gt;(\u0026#39;start\u0026#39;); const [difficulty, setDifficulty] = useState\u0026lt;DifficultyLevel\u0026gt;(\u0026#39;normal\u0026#39;); const handleGameStart = useCallback((selectedDifficulty: DifficultyLevel) =\u0026gt; { setDifficulty(selectedDifficulty); setGamePhase(\u0026#39;playing\u0026#39;); }, []); if (gamePhase === \u0026#39;start\u0026#39;) { return \u0026lt;StartScreen onStart={handleGameStart} /\u0026gt;; } return \u0026lt;GameUI difficulty={difficulty} /\u0026gt;; } Common Questions Why use useState instead of regular variables? Regular variables don\u0026rsquo;t trigger re-renders when changed. The setState function returned by useState triggers component re-renders to update the UI.\nWhat is the useEffect dependency array? The dependency array tells React when to re-run the effect:\n[] empty array: Run only on mount [count]: Run when count changes Not provided: Run on every render Learning Resources React Official Docs React New Tutorial Last Updated: 2025-11-18\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/react-fundamentals/","summary":"Core React concepts applied to building game interfaces - components, state, hooks, and more.","title":"React Fundamentals for Game UI"},{"content":"Overview This project represents a solitary yet enlightening journey into Embodied AI. The goal: create NPCs that can truly perceive, reason, and act in a 3D game environment.\nDevelopment Timeline Phase 1: LLM-Driven NPCs (November 2025) Initial Approach: Use Large Language Models to drive NPC behavior in Unity.\nMethod: Heuristic hacks like Raycast for perception\nResult: Poor navigation and lack of true spatial awareness\nLessons Learned:\nText-only LLMs cannot truly understand 3D space Raycast-based perception is brittle and limited Need a way for AI to \u0026ldquo;see\u0026rdquo; the environment ML-Agents Detour Attempt: Use Unity\u0026rsquo;s ML-Agents for reinforcement learning\nRealization: RL creates a \u0026ldquo;conditioned subject\u0026rdquo; - functional but incapable of generalization\nProblem: Training environment-specific behaviors doesn\u0026rsquo;t transfer to new scenarios\nPhase 2: VLM Integration (December 2025) Breakthrough: Integration of Vision-Language Models (VLM)\nKey Innovation: The agent can now truly \u0026ldquo;see\u0026rdquo; by processing camera frames\nArchitecture:\nUnity captures game view VLM processes visual input LLM reasons about observations Actions sent back to Unity Phase 3: End-to-End Loop (January 2026) Achievement: Complete perception-reasoning-action loop\nCapabilities:\nPerceives environment through vision Reasons about observations Reacts with specific behaviors and emotions Facial expressions and walking animations Avatar that can interact with players Current Limitations:\nHardware constraints require script-driven action layer Full VLA (Vision-Language-Action) implementation not yet possible Models and scenes still rough Some bugs to overcome Technical Architecture The system follows a multi-layer design:\nPerception Layer: Camera capture ‚Üí VLM processing Cognition Layer: LLM reasoning about visual observations Action Layer: Script-driven behaviors (temporary solution) Expression Layer: Emotion and animation control Challenges Encountered 3D Embodiment Implementing embodiment in 3D scenes proved much more challenging than anticipated:\nWalking navigation Database setup for knowledge persistence Real-time communication between Unity and AI backend Hardware Constraints Current hardware doesn\u0026rsquo;t support full end-to-end VLA models, requiring a hybrid approach with scripted action execution.\nFuture Directions World Models: Explore predictive models for better planning Sim2Real Robotics: Transfer learnings to physical robots New Game Scenes: Implement complete game designs Open Theater Game Theory: Use this NPC system to support interactive narrative experiments Reflections This prototype validates the core architecture for embodied AI in games. While not yet a complete VLA system, it demonstrates that:\nVision gives AI true spatial awareness LLM reasoning can drive meaningful behavior Emotion and expression add depth to interactions The basic requirements are now met, and future game designs can be realized in new scenes, providing support for the Open Theater Game Theory.\nLast Updated: 2025-01-10\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/projects/my-embodied-ai-journey/","summary":"A chronicle of building intelligent NPCs in Unity, from heuristic hacks to vision-language models.","title":"My Embodied AI Development Journey"}]