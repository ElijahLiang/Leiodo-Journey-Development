[{"content":"Introduction This project represents a solitary yet enlightening journey into Embodied AI. Starting in November 2025, I attempted to drive an NPC using LLMs within Unity. What began as a simple experiment evolved into a deep exploration of how AI can truly \u0026ldquo;see\u0026rdquo; and understand its environment.\nThe Beginning: Heuristic Approaches My initial approach relied on heuristic hacks like Raycast for perception:\n// Traditional approach - Raycast-based perception RaycastHit hit; if (Physics.Raycast(transform.position, transform.forward, out hit, maxDistance)) { string objectName = hit.collider.gameObject.name; // Convert to text description for LLM } This resulted in:\n❌ Poor navigation ❌ Lack of true spatial awareness ❌ Brittle behavior trees I briefly detoured into ML-Agents, only to realize that reinforcement learning created a \u0026ldquo;conditioned subject\u0026rdquo; — functional but incapable of the generalization I sought.\nThe Breakthrough: Vision-Language Models The breakthrough came in December with the integration of Vision-Language Models (VLM). By connecting a first-person camera to Qwen-VL-Plus, the agent gained the ability to truly \u0026ldquo;see\u0026rdquo;:\nP C E a R m C e E r P a T I → O N R e P n I d P e E r L T I e N x E t : u r e → P N G B y t e s → B a s e 6 4 → V L M A P I → S c e n e D e s c r i p t i o n The VLM doesn\u0026rsquo;t just detect objects — it understands spatial relationships, context, and even subtle visual cues.\nThe Current State: End-to-End Loop In January 2026, I successfully achieved an end-to-end loop:\nPerceive: VLM interprets first-person visual input Reason: LLM processes scene description with context Act: Agent executes specific behaviors Express: Emotional responses through animations Architecture Overview ┌ │ │ └ ─ ─ ─ ─ ─ U 5 ─ ─ n 1 ─ ─ i 2 ─ ─ t x ─ ─ y 5 ─ ─ 1 ─ ─ C 2 ─ ─ a ─ ─ m P ─ ─ e N ─ ─ r G ─ ─ a ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┘ ─ ─ ─ ▶ ┌ │ │ └ ─ ─ ─ ─ ─ Q S ─ ─ w c ─ ─ e e ─ ─ n n ─ ─ - e ─ ─ V ─ ─ L A ─ ─ n ─ ─ ( a ─ ─ V l ─ ─ L y ─ ─ M s ─ ─ ) i ─ ─ s ─ ─ ─ ─ ─ ┐ │ ┘ ─ │ ─ ─ ▶ ┌ │ │ │ └ ┌ │ └ ─ ─ ─ │ ─ ─ ─ ─ ─ ─ A G S ─ ─ D ─ ─ c O T ─ ─ e D ─ ─ t _ O ─ ─ e e ─ ─ i T P ─ ─ p c ─ ─ o O ─ ─ S i ─ ─ n E ─ ─ e s ─ │ ▼ ─ E M ─ ─ e i ─ ─ L X O ─ ─ k o ─ ─ a P T ─ ─ n ─ ─ y L I ─ ─ L ─ ─ e O O ─ ─ L ─ ─ r R N ─ ─ M ─ ─ E ─ ─ ─ ─ ─ ─ ─ ┐ │ │ │ ┘ ┐ │ ┘ │ Key Learnings 1. Perception is Everything Without true visual understanding, AI agents are fundamentally limited. VLMs bridge the gap between raw pixels and semantic understanding.\n2. Memory Matters The agent maintains three types of memory:\nEnvironmental: Known objects, visited places Dialogue: Conversation history Goals: Current task, sub-goals 3. Emergent Behaviors With proper perception, behaviors emerge naturally:\nCuriosity-driven exploration Memory-informed avoidance Context-aware responses What\u0026rsquo;s Next While hardware constraints currently necessitate a script-driven action layer (preventing a full VLA implementation), this prototype validates the core architecture. Future explorations include:\nWorld Models: Predicting future states Sim2Real: Transferring to physical robots Multi-agent: Collaborative AI behaviors Conclusion This journey taught me that the key to intelligent agents isn\u0026rsquo;t more complex behavior trees — it\u0026rsquo;s giving them the ability to truly perceive and understand their world. The combination of VLM + LLM provides a powerful foundation for creating agents that feel genuinely intelligent.\nThis article is part of my Embodied Intelligence Project. Check out the demos and technical documentation there.\n","permalink":"https://ElijahLiang.github.io/Leiodo-Journey-Development/posts/my-embodied-ai-journey/","summary":"A reflection on my exploration of Embodied AI - from heuristic hacks to vision-language models.","title":"My Journey into Embodied AI: From Raycast to VLM"}]